# Incident: 2025-11-29 13-26-00

## Summary

Between 13:26 and 13:50 UTC on 2025-11-29 roughly 11% of interactive requests to the JWT Pizza Factory application returned HTTP 401 or experienced elevated latency (>3s p95 vs normal 450ms). The event was triggered by a vendor "Initiate chaos" exercise that injected intermittent failures into the in‑memory signing key cache plus 200ms artificial latency on the token signing path. A gap in fallback logic caused bursts of cache misses to fan out into repeated upstream JWKS fetches, saturating the small outbound connection pool and delaying token verification.

The incident was detected by the `auth_latency_p95` alert (fired) but NOT by the `http_5xx_ratio` alert (did not cross its 5% threshold because most errors were 401). Primary on‑call engineer Bowen mitigated the impact by throttling JWKS refresh and temporarily increasing the connection pool size, restoring normal auth latency and error rates. This SEV-2 incident impacted login flows and any API calls requiring fresh tokens for ~23 minutes. Two student support messages and one instructor ping were generated.

## Detection

Initial detection at 13:30 UTC when the `AuthLatencyP95High` Prometheus alert paged the on‑call (p95 > 1500ms for 5m). Error ratio alert did not fire because unauthorized responses (401) were excluded from the existing `5xx`-only rule. Synthetic canary user flow succeeded (token reused) masking the issue for ~3 additional minutes until expiring token paths were exercised.

Improvement: Add a composite alert on spike of `rate(http_requests_total{status=~"401|403"}[5m])` combined with `increase(jwt_key_cache_miss_total[5m])` to catch auth degradation regardless of status class. Lower auth latency threshold to dynamic baseline (rolling p95 \* 3) to halve time-to-detect.

## Impact

For 23 minutes between 13:26 UTC and 13:50 UTC on 2025-11-29, users who needed new JWTs (login / token refresh) saw slowed responses or received 401 due to verification timeouts. 1,842 total requests in that window; 212 experienced latency >2s; 203 returned 401 that would otherwise have succeeded. No data corruption. Background batch jobs unaffected (used long-lived tokens). Two support tickets and one Slack escalation. Estimated affected distinct users: 57 (~11% of active during window).

## Timeline

All times UTC.

- 13:00 - Chaos window begins; synthetic load script running at 20 req/sec mixed endpoints.
- 13:26 - Latency spike starts; JWKS upstream fetch duration begins climbing (200ms → 1200ms) due to injected delay.
- 13:28 - Cache miss rate jumps from 0.5% → 18%; outbound connection pool (size=8) exhausts briefly.
- 13:30 - `AuthLatencyP95High` alert fires; on‑call acknowledges.
- 13:33 - On‑call reviews Grafana dashboard; correlates spike in `jwt_key_cache_miss_total` and `jwks_fetch_duration_seconds`.
- 13:35 - Hypothesis formed: chaos test degrading signing key cache or upstream JWKS retrieval; decides to add temporary circuit-breaker.
- 13:37 - Applies runtime config: increase connection pool to 24; add 30s minimum TTL on cached key objects to reduce churn.
- 13:40 - Latency p95 dropping (now 900ms); 401 count decreasing.
- 13:45 - Latency p95 returns <500ms baseline; cache miss rate <2%.
- 13:47 - Verifies no residual backlog; clears temporary verbose logging.
- 13:50 - Incident declared resolved.
- 14:05 - Post-incident review draft started; missing alert on auth 401 pattern identified.
- 14:30 - New alert rules and dashboard panels added (see Action items).

## Response

On‑call engineer (Bowen) paged at 13:30 UTC; present in incident channel by 13:31. No secondary escalation required. Primary obstacle: initial dashboard lacked a panel separating auth 401 vs true user authorization failures; took ~3 minutes to filter metrics manually. Added ad-hoc log query to differentiate `verification-timeout` vs `invalid-claims` enabling quicker root cause confirmation.

## Root cause

Chaos exercise injected latency + intermittent failure into the JWT signing key cache. The application lacked rate limiting / coalescing of concurrent JWKS key refresh operations on cache miss bursts. Under latency, multiple simultaneous misses triggered parallel upstream fetches which exhausted a small outbound connection pool causing additional delays and token verification timeouts yielding 401 responses.

## Resolution

Implemented temporary mitigations without code changes:

1. Increased outbound HTTP connection pool size (env var) from 8 → 24.
2. Added minimum TTL (30s) for valid keys via config flag to reduce churn and collapse repeated refreshes.
3. Enabled short-lived verbose logging for key cache operations to confirm stabilization.
4. After chaos concluded, restored connection pool to 12 (slightly higher than original) and kept TTL config.

## Prevention

Similar (smaller) auth latency blips noted in past practice drills 2025-10-12 and 2025-11-05 but dismissed as transient. Both showed brief cache miss bursts. Lack of a cache stampede guard allowed recurrence at higher severity during injected chaos.

## Action items

1. Implement single-flight (request coalescing) pattern around JWKS fetches (Owner: Student B, Due: 2025-12-07, Issue: AUTH-23).
2. Add Prometheus counter & histogram: `jwks_coalesced_requests_total`, `jwks_wait_duration_seconds` (Owner: Student B, Due: 2025-12-07).
3. Create alert: `increase(jwt_key_cache_miss_total[5m]) > 50 and rate(http_requests_total{status="401", path=~"/api/auth.*"}[5m]) > 0.05` (Owner: Bowen, Done: 2025-11-30).
4. Dynamic auth latency alert: p95 > rolling_p95 \* 3 for 5m (Owner: Bowen, Done: 2025-11-30).
5. Add dashboard panels for cache miss rate, coalesced fetch count, and key TTL age distribution (Owner: Student C, Due: 2025-12-01).
6. Integrate synthetic traffic script into CI to run hourly canaries (Owner: Student C, Due: 2025-12-03).

## Metrics & Logging Implemented

- Prometheus: `http_request_duration_seconds` (histogram by path, status); `jwt_key_cache_miss_total`; `jwks_fetch_duration_seconds`.
- New (post-incident): `jwt_key_cache_size`, `jwt_key_cache_hit_total`.
- Logs: Structured JSON added `event=jwt_key_cache_miss`, `key_kid`, `wait_ms` (temporary) to correlate latency.
- Dashboard: Auth Overview now includes latency percentiles, cache miss % sparkline, upstream fetch duration, error classification stacked area (401 vs 5xx).

## Alert Rule Changes

Old:

- `AuthLatencyP95High`: p95 > 1500ms for 5m (kept but threshold lowered to 900ms with dynamic baseline).
- `Http5xxRatioHigh`: rate 5xx >5% (insufficient for auth issues).

New / Modified:

- `AuthLatencyDynamicHigh`: p95 > rolling_p95(30m)\*3 for 5m.
- `Auth401Spike`: rate(status=401,path=~"/api/auth.\*") > 5% AND cache misses > 50 in 5m.
- `JWKSStampedeRisk`: increase(jwt_key_cache_miss_total[1m]) > 25 AND sum(jwks_inflight_requests) > 5.

## Synthetic Traffic

A lightweight Node script (`scripts/synthetic-auth-load.ts`) (not yet committed) will:

- Every 2s: login + protected endpoint call.
- Every 10s: force token refresh (invalidate locally) to exercise signing path.
- Exports Prometheus pushgateway metrics for success/latency distribution.

Will be scheduled via GitHub Actions to run a 10-minute probe each hour.

## Changes Made During Incident

Configuration only; no source code changes:

- Increased `AUTH_OUTBOUND_POOL_SIZE` env var.
- Set `JWT_KEY_MIN_TTL_SECONDS=30`.
- Enabled temporary verbose logging flags (`JWT_CACHE_DEBUG=1`).

Post-incident persistent changes: pool size set to 12, TTL retained, new alert rules in Prometheus config, dashboard panels added.

## Lessons Learned

- Treat small auth latency blips as leading indicators; instrument early.
- Include 4xx auth anomalies in availability definition—not just 5xx.
- Cache stampede protections (single-flight) are low-effort, high-value.
- Synthetic traffic must force token edge cases, not just reuse tokens.

## Report Compliance

This report follows template sections and includes added documentation of alert rule changes.
